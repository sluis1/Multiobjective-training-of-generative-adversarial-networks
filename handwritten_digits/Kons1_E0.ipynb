{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This is an example for a GAN handwritten digits generation\n",
    "# The optimization is done with the accelerated multiobjective gradient method\n",
    "# Factor here is set to 1000 instead of 2\n",
    "#\n",
    "\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils.layer_utils import count_params  \n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow as tf\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import csv\n",
    "from scipy.linalg import norm\n",
    "from scipy.optimize import minimize, LinearConstraint\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# function to set seeds for the imports to repeat results\n",
    "#\n",
    "\n",
    "def set_seed(tmp_seed):\n",
    "    np.random.seed(tmp_seed)\n",
    "    tf.random.set_seed(tmp_seed)\n",
    "    torch.manual_seed(tmp_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# function to creat folder for saving results\n",
    "#\n",
    "\n",
    "def create_folder(tmp_seed):\n",
    "\n",
    "    # Set the path the results should be saved in\n",
    "    SAVEPATH = R'saved_networks/Kons1_E0/_seed_'\n",
    "    \n",
    "    # create folder if not exist\n",
    "    if os.path.exists(os.path.normpath(SAVEPATH +str(tmp_seed))):\n",
    "        shutil.rmtree(os.path.normpath(SAVEPATH +str(tmp_seed)))\n",
    "        os.makedirs(os.path.normpath(SAVEPATH +str(tmp_seed)))\n",
    "    else:\n",
    "        os.makedirs(os.path.normpath(SAVEPATH +str(tmp_seed)))\n",
    "\n",
    "    return SAVEPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# function to prepare the training set\n",
    "#\n",
    "\n",
    "def prepare(batch_size):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    "    )\n",
    "\n",
    "    train_set = torchvision.datasets.MNIST(\n",
    "        root=\"../\", train=True, download=False, transform=transform\n",
    "    )\n",
    "\n",
    "    indices = train_set.targets==3\n",
    "    train_set.data , train_set.targets = train_set.data[indices], train_set.targets[indices]\n",
    "\n",
    "    # 6131 is len of data. with batch size 32 this is not possible, so 6131/32 = 191.59 -> 191 executions\n",
    "    train_set.data = train_set.data[:6112]\n",
    "    train_set.targets = train_set.targets[:6112]\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This cell is for drawing the image\n",
    "# if the parameter save is set, the images are also saved\n",
    "#\n",
    "\n",
    "def view_samples(samples,save,path,tmp_seed,epoch):\n",
    "    for i in range(16):\n",
    "        ax = plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(samples[i].reshape(28, 28), cmap=\"gray_r\")\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    if(save):\n",
    "        if not os.path.isdir(os.path.normpath(path +str(tmp_seed)+R'/plots')):\n",
    "            os.makedirs(os.path.normpath(path +str(tmp_seed)+R'/plots/pdf'))\n",
    "            os.makedirs(os.path.normpath(path +str(tmp_seed)+R'/plots/png'))\n",
    "            #os.makedirs(os.path.normpath(path +str(tmp_seed)+R'/plots/png/image_values'))\n",
    "            os.makedirs(os.path.normpath(path +str(tmp_seed)+R'/plots/networks'))\n",
    "        plt.savefig(os.path.normpath(path +str(tmp_seed)+R'/plots/pdf/generated_images_e%d.pdf'%epoch), bbox_inches='tight')\n",
    "        plt.savefig(os.path.normpath(path +str(tmp_seed)+R'/plots/png/generated_images_e%d.png'%epoch), bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This cell defines the discriminator network which is a network\n",
    "# input size: 784 (shape to (None,1,28,28))\n",
    "# hidden layer: 512\n",
    "# output size: 1\n",
    "#\n",
    "\n",
    "def setup_dis():\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(tf.keras.layers.Reshape((784,), input_shape=(1,28,28)))\n",
    "    discriminator.add(Dense(512, activation='relu'))\n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "    org_weights_dis = discriminator.get_weights()\n",
    "    discriminator.compile(loss= tf.keras.losses.BinaryCrossentropy(), optimizer=optimizers.SGD(learning_rate=0.01),metrics=[\"accuracy\"])\n",
    "    \n",
    "    return discriminator, org_weights_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This cell defines the generator network which is a network\n",
    "# input size: 10\n",
    "# hidden layer: 512\n",
    "# output size: 784 (shape to (None,1,28,28))\n",
    "#\n",
    "\n",
    "def setup_gen():\n",
    "    generator = Sequential()\n",
    "    generator.add(Dense(512, input_dim=10, activation='relu'))\n",
    "    generator.add(Dense(784, activation='tanh'))\n",
    "    generator.add(tf.keras.layers.Reshape((1, 28, 28)))\n",
    "    org_weights_gen = generator.get_weights()\n",
    "\n",
    "    return generator, org_weights_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This cell defines the GAN network which combines the Generator and the Discriminator.\n",
    "# In case we dont want to train the Discriminator when we train the Generator we have to set the discriminator.trainable to False\n",
    "#\n",
    "\n",
    "def setup_gan(discriminator, generator):\n",
    "    \n",
    "    discriminator.trainable = False\n",
    "    GAN = Sequential()\n",
    "    GAN.add(generator)\n",
    "    GAN.add(discriminator)\n",
    "    GAN.compile(loss=tf.keras.losses.BinaryCrossentropy() , optimizer=optimizers.SGD(learning_rate=0.01),metrics=[\"accuracy\"])\n",
    "\n",
    "    return GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This is the function we want to minimize which is || h*( sum(xi * nabla fi(y^k))- ((k-1)/(k+1000)*(x^k - x^(k-1)))||^2\n",
    "#\n",
    "\n",
    "def f(x,params):\n",
    "    A, fac, h = params\n",
    "    return norm((h*(x[0]*A[0,:] + x[1] * A[1,:])) - fac )**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This is the function which call minimize function with the constraints.\n",
    "# The constraints are xi>0 and sum(xi) = 1 \n",
    "#\n",
    "\n",
    "def minimization(A, fac, h):\n",
    "    # this is the initial guess, here set to 1\n",
    "    initial_guess = np.ones(2)\n",
    "\n",
    "    # this is a Matrix [[1,0],[0,1]]\n",
    "    B = np.zeros((2,2))\n",
    "    B[0,0] = 1\n",
    "    B[1,1] = 1\n",
    "\n",
    "    # the lower bound verifies that xi >0 \n",
    "    lb = np.array([0,0])\n",
    "\n",
    "    # one constraint is for xi>0 and one for sum(xi) = 1\n",
    "    const = [LinearConstraint(B,lb,np.inf), {'type':'eq','fun': lambda x:  np.sum(x)-1.0}]\n",
    "\n",
    "    # minimize the function f with A factor and h\n",
    "    res = minimize(f, initial_guess, method = 'SLSQP' , args= [A, fac, h], constraints = const, options={'ftol': 1e-6,'disp': False}) # here true for display\n",
    "\n",
    "    # return the result\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This function calculates the gradients for the minimization problem\n",
    "# For the jacobian Matrix here the off diagonal is set to 0\n",
    "# The result is A = [[ED_D,0],[0,EG_G]]\n",
    "#\n",
    "\n",
    "def calculate_gradients(gan_model,gen,d_model, z, x_real, y_real,y_fake,batch_size,EG_wD,ED_wG):\n",
    "\n",
    "    z = np.vstack(z)\n",
    "    x_real = np.vstack(x_real).reshape(batch_size,1,28,28)\n",
    "\n",
    "    # set the loss function for the GAN model to BinaryCrossentropy()\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    # Set all gradients to 0\n",
    "    d_model.trainable = True\n",
    "    \n",
    "    # Calculate gradients with respect to every trainable variable for ED_D\n",
    "    # must set the trainable weights to True to get results because otherwise the number of weights is 0\n",
    "    EG_wG = []\n",
    "    ED_wD = [] \n",
    "    with tf.GradientTape() as tape0, tf.GradientTape() as tape1:\n",
    "        gan_pred = gan_model(z)\n",
    "        loss1 = bce(y_real, gan_pred)\n",
    "\n",
    "        dis_pred = d_model(x_real)\n",
    "        X,Y = tf.reshape(tf.stack((dis_pred,gan_pred)),shape=(2*batch_size,1)), tf.reshape(tf.stack((y_real,y_fake)),shape=(2*batch_size,1))\n",
    "        loss2 = bce(Y,X)\n",
    "\n",
    "    EG_G = tape0.gradient(loss1, gen.trainable_weights)\n",
    "    ED_D = tape1.gradient(loss2, d_model.trainable_weights)\n",
    "    for i in range(len(ED_D)):\n",
    "        ED_wD.extend(ED_D[i].numpy().reshape(-1))\n",
    "    for i in range(len(EG_G)):\n",
    "        EG_wG.extend(EG_G[i].numpy().reshape(-1))\n",
    "\n",
    "    d_model.trainable = False\n",
    "\n",
    "    # set the first row the Error of the Discriminator [ED_wD,0]\n",
    "    ED = []\n",
    "    ED.extend(ED_wD)\n",
    "    ED.extend(ED_wG)\n",
    "\n",
    "    # set the second row the Error of the Generator [0,ED_wG]\n",
    "    EG = []\n",
    "    EG.extend(EG_wD)\n",
    "    EG.extend(EG_wG)\n",
    "\n",
    "\n",
    "    # calculate jacobian matrix\n",
    "    \n",
    "    return np.array([ED,EG])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# prepare training by setting parameters\n",
    "# every digit will be trained 30 times. -> 30 num_epochs * len_trainloader\n",
    "#\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "len_trainloader = 191\n",
    "number_calculations = len_trainloader*num_epochs\n",
    "\n",
    "\n",
    "# set up the label for fake or real images\n",
    "# labels = 0 for discriminator for fake images\n",
    "# labels = 1 for discriminator for real images and generator for fake images\n",
    "real_samples_labels = np.ones((batch_size, 1))\n",
    "generated_samples_labels = np.zeros((batch_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   Train function to train the GAN\n",
    "#\n",
    "\n",
    "def train(discriminator, generator, GAN, org_weights_dis,org_weights_gen, SAVEPATH, tmp_seed,train_loader,EG_wD,ED_wG):\n",
    "\n",
    "    # setup learning rate and decrease factor\n",
    "    learning_rate = 0.01\n",
    "    alpha = 0.99\n",
    "\n",
    "    # setup the lists to plot the error after training\n",
    "    error_discriminator = []\n",
    "    error_generator = []\n",
    "    \n",
    "    # set the start time\n",
    "    st = time.time()\n",
    "\n",
    "    # reset weights for each training\n",
    "    generator.set_weights(org_weights_gen)\n",
    "    discriminator.set_weights(org_weights_dis)\n",
    "\n",
    "    # set the at first x^(k-1) to x^k which are the actual weights\n",
    "    x_gk_m1 = generator.get_weights()\n",
    "    x_dk_m1 = discriminator.get_weights()\n",
    "\n",
    "    # set k to 1\n",
    "    k = 1\n",
    "\n",
    "    # process the minimization until norm(v) < 1e-4 or we have reached number minimization steps in case this is also done for alternating training\n",
    "    while (True):\n",
    "        # use next samples of trainloader\n",
    "        real_samples, mnist_labels = next(iter(train_loader))\n",
    "\n",
    "        # generate batch_size many inputs and fake images\n",
    "        latent_space_samples = torch.randn((batch_size, 10))\n",
    "\n",
    "        # set x_k for the generator and the discriminator\n",
    "        x_gk = generator.get_weights()\n",
    "        x_dk = discriminator.get_weights()\n",
    "\n",
    "\n",
    "        # y^k = x^k + ((k-1)/(k+1000)*(x^k-x^(k-1))) fpr discriminator and generator\n",
    "        # for the discriminator\n",
    "        # need first save as tmp because change directly the weights\n",
    "        x_dk_tmp = discriminator.get_weights()\n",
    "        x_gk_tmp = generator.get_weights()\n",
    "\n",
    "        for i in range(len(x_dk_tmp)):\n",
    "            x_dk_tmp[i] = x_dk_tmp[i] + ((k-1)/(k+1000)*(x_dk_tmp[i]-x_dk_m1[i])) \n",
    "        discriminator.set_weights(x_dk_tmp)\n",
    "        # for the generator\n",
    "        for i in range(len(x_gk_tmp)):\n",
    "            x_gk_tmp[i] = x_gk_tmp[i] + ((k-1)/(k+1000)*(x_gk_tmp[i]-x_gk_m1[i]))\n",
    "        generator.set_weights(x_gk_tmp) \n",
    "\n",
    "        # calculate jacobian matrix\n",
    "        A = calculate_gradients(GAN,generator,discriminator, latent_space_samples, real_samples, real_samples_labels, generated_samples_labels,batch_size,EG_wD,ED_wG)\n",
    "        # calculate factor = (((k-1)/(k+1000))*(x_k-x_k_m1))) for generator and discriminator\n",
    "        # this factor is used in minimization problem\n",
    "        factor = []\n",
    "        for i in range(len(x_dk)):\n",
    "            factor.extend((k-1)/(k+1000)*(x_dk[i].reshape(-1)-x_dk_m1[i].reshape(-1)))\n",
    "        for i in range(len(x_gk)):\n",
    "            factor.extend((k-1)/(k+1000)*(x_gk[i].reshape(-1)-x_gk_m1[i].reshape(-1)))\n",
    "        # solve the minimization problem\n",
    "        theta = minimization(A, factor, learning_rate)\n",
    "\n",
    "        # now get the v to update the weights by calculate the sum of theta*A\n",
    "        v = theta.x[0]*A[0,:] + theta.x[1]*A[1,:]\n",
    "\n",
    "        # split v into v_discriminator and v_generator to update the weights for each network\n",
    "        # in case that A is first row is ED and second row is EG first v is for weights of the discriminator\n",
    "        discriminator.trainable = True\n",
    "        v_dis = v[:count_params(discriminator.trainable_weights)]\n",
    "        v_gen = v[count_params(discriminator.trainable_weights):]\n",
    "        discriminator.trainable = False\n",
    "\n",
    "        # set x_k_m1 to x_k\n",
    "        # this is for the next iteration to have x^(k-1) = x^k in the next iteration\n",
    "        x_gk_m1 = x_gk\n",
    "        x_dk_m1 = x_dk\n",
    "\n",
    "        # update the weights for the the discriminator and generator\n",
    "        wd = discriminator.get_weights()\n",
    "        for i in range(len(wd)):\n",
    "            wd[i] = wd[i] - (learning_rate* v_dis[:wd[i].size]).reshape(wd[i].shape)\n",
    "            v_dis = v_dis[wd[i].size:]\n",
    "        discriminator.set_weights(wd)\n",
    "\n",
    "        wg = generator.get_weights()\n",
    "        for i in range(len(wg)):\n",
    "            wg[i] = wg[i] - (learning_rate* v_gen[:wg[i].size]).reshape(wg[i].shape)\n",
    "            v_gen = v_gen[wg[i].size:]\n",
    "        generator.set_weights(wg)\n",
    "\n",
    "        # if the norm is smaller than 1e-4 or the number of iterations greater than 1000 stop\n",
    "        if (k >=number_calculations):\n",
    "            break\n",
    "\n",
    "        # Show loss\n",
    "        if((k)%len_trainloader==0):\n",
    "            epoch = k/len_trainloader\n",
    "            generated_samples = generator.predict(np.vstack(latent_space_samples),verbose=0)\n",
    "            all_samples,all_samples_labels = np.vstack((real_samples, generated_samples)),np.vstack((real_samples_labels, generated_samples_labels))\n",
    "            d_loss, _ = discriminator.evaluate(all_samples, all_samples_labels, verbose=0)\n",
    "            g_loss,_ = GAN.evaluate(np.vstack(latent_space_samples), real_samples_labels,verbose=0)\n",
    "\n",
    "            print(f\"Epoch: {epoch} Loss D.: {d_loss}\")\n",
    "            print(f\"Epoch: {epoch} Loss G.: {g_loss}\")\n",
    "            with open(os.path.normpath(SAVEPATH +str(tmp_seed)+'/performance.txt'), \"a+\") as outfile:\n",
    "                # summarize discriminator performance\n",
    "                outfile.write(f\"Epoch: {epoch} Loss D.: {d_loss}\"+\"\\n\")\n",
    "                outfile.write(f\"Epoch: {epoch} Loss G.: {g_loss}\"+\"\\n\")\n",
    "                \n",
    "            generated_samples = generator.predict(np.vstack(latent_space_samples),verbose=0)\n",
    "\n",
    "            view_samples(generated_samples,True,SAVEPATH,tmp_seed,epoch)\n",
    "\n",
    "            #decrease learning_rate\n",
    "            learning_rate = learning_rate*alpha\n",
    "\n",
    "            # reset trainloader\n",
    "            train_loader = prepare(batch_size)\n",
    "        # increase k\n",
    "        k +=1\n",
    "\n",
    "    # get the end time\n",
    "    et = time.time()\n",
    "\n",
    "    # get the execution time\n",
    "    elapsed_time = et - st\n",
    "\n",
    "    # evaluate the loss after training\n",
    "    latent_space_samples = torch.randn((batch_size, 10))\n",
    "\n",
    "    generated_samples = generator.predict(np.vstack(latent_space_samples),verbose=0)\n",
    "\n",
    "    all_samples,all_samples_labels = np.vstack((real_samples, generated_samples)),np.vstack((real_samples_labels, generated_samples_labels))\n",
    "\n",
    "    d_loss, _ = discriminator.evaluate(all_samples, all_samples_labels, verbose=0)\n",
    "    g_loss,_ = GAN.evaluate(np.vstack(latent_space_samples), real_samples_labels,verbose=0)\n",
    "    \n",
    "    epoch = (k-1)/len_trainloader\n",
    "    # print performance of last run\n",
    "    print(f\"Epoch: {epoch} Loss D.: {d_loss}\")\n",
    "    print(f\"Epoch: {epoch} Loss G.: {g_loss}\")\n",
    "    with open(os.path.normpath(SAVEPATH +str(tmp_seed)+'/performance.txt'), \"a+\") as outfile:\n",
    "        # summarize discriminator performance\n",
    "        outfile.write(f\"Epoch: {epoch} Loss D.: {d_loss}\"+\"\\n\")\n",
    "        outfile.write(f\"Epoch: {epoch} Loss G.: {g_loss}\"+\"\\n\")\n",
    "\n",
    "    # append the loss for plotting the loss for each lambda_weighted\n",
    "    error_generator.append(g_loss)\n",
    "    error_discriminator.append(d_loss)\n",
    "\n",
    "    generated_samples = generator.predict(np.vstack(latent_space_samples))\n",
    "\n",
    "    view_samples(generated_samples,True,SAVEPATH,tmp_seed,epoch)\n",
    "\n",
    "    # save the weights of the generator and discriminator\n",
    "    generator.save(os.path.normpath(SAVEPATH +str(tmp_seed)+'/plots/networks/generator.h5'))\n",
    "    discriminator.save(os.path.normpath(SAVEPATH +str(tmp_seed)+'/plots/networks/discriminator.h5'))\n",
    "        \n",
    "    return error_generator, error_discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of initializations\n",
    "number_seeds = 3\n",
    "\n",
    "\n",
    "all_errors_gen = []\n",
    "all_errors_dis = []\n",
    "start_alltime = time.time()\n",
    "\n",
    "# train GAN for each initialization\n",
    "for i in range(number_seeds):\n",
    "    # generate and set seed\n",
    "    seed = np.random.randint(99999999)\n",
    "    set_seed(seed)\n",
    "\n",
    "    # create path\n",
    "    path = create_folder(seed)\n",
    "\n",
    "    # setup images for this seed\n",
    "    train_loader = prepare(batch_size)\n",
    "\n",
    "    # setup weights for this seed\n",
    "    dis,org_dis = setup_dis()\n",
    "    gen,org_gen = setup_gen()\n",
    "    gan = setup_gan(dis,gen)\n",
    "\n",
    "    dis.trainable = True\n",
    "    EG_wD = np.zeros((count_params(dis.trainable_weights), 1)).reshape(-1)\n",
    "    ED_wG = np.zeros((count_params(gen.trainable_weights), 1)).reshape(-1)\n",
    "    dis.trainable = False\n",
    "\n",
    "    # train the GAN for specific initialization\n",
    "    error_generator, error_discriminator = train(dis,gen,gan, org_dis, org_gen,path,seed,train_loader,EG_wD,ED_wG)\n",
    "    all_errors_gen.extend(error_generator)\n",
    "    all_errors_dis.extend(error_discriminator)\n",
    "\n",
    "# save computation time of all seeds\n",
    "end_alltime = time.time()\n",
    "\n",
    "# get the execution time\n",
    "computation_time = end_alltime - start_alltime\n",
    "time_string = '>>>>>>>Complete computatiom time: %.3f seconds<<<<<<<' %np.round(computation_time,3)\n",
    "print(time_string)\n",
    "\n",
    "# save computatuion time\n",
    "with open(os.path.normpath(path +str(seed) +'/../performance.txt'), \"a+\") as outfile:   \n",
    "    outfile.write(time_string+\"\\n\"+\"\\n\")\n",
    "\n",
    "# plot the gan errors for all seeds    \n",
    "plt.scatter([all_errors_gen], [all_errors_dis], color= 'red',s = 20,cmap='RdYlGn_r', vmin=0, vmax=3)\n",
    "plt.colorbar(label=\"image evaluation\")\n",
    "plt.title(\"GAN loss over seeds\")\n",
    "plt.xlabel('Generator_loss', fontsize=18)\n",
    "plt.ylabel('Discriminator_loss', fontsize=16)\n",
    "plt.savefig(os.path.normpath(path +str(seed)+'/../GAN_error_over_seeds.pdf')) \n",
    "plt.close()\n",
    "\n",
    "#\n",
    "# plot and save the losses for all seeds\n",
    "#\n",
    "\n",
    "plt.scatter([all_errors_gen], [all_errors_dis], color= 'red',s = 20,cmap='RdYlGn_r', vmin=0, vmax=3)\n",
    "plt.colorbar(label=\"image evaluation\")\n",
    "plt.xscale('symlog')\n",
    "plt.yscale('symlog')\n",
    "plt.title(\"GAN loss over seeds in log scale\")\n",
    "plt.xlabel('Generator_loss', fontsize=18)\n",
    "plt.ylabel('Discriminator_loss', fontsize=16)\n",
    "plt.savefig(os.path.normpath(path +str(seed)+'/../GAN_error_over_seeds_logscale.pdf'))\n",
    "plt.close()\n",
    "\n",
    "#\n",
    "# save the erros into csv file to reuse them\n",
    "#\n",
    "\n",
    "with open(os.path.normpath(path +str(seed)+'/../saved_errors.csv'), 'w') as f:\n",
    "    # using csv.writer method from CSV package\n",
    "    write = csv.writer(f)\n",
    "    write.writerows([all_errors_gen])\n",
    "    write.writerows([all_errors_dis])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e11b55361148b6c3db81e2c737cd36da1a961dccb25eb888a6a7c970ac9dd9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
