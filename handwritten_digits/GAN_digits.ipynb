{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This is an example for a GAN handwritten digits generation\n",
    "# The optimization is done with alternating training\n",
    "#\n",
    "\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils.layer_utils import count_params  \n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow as tf\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import csv\n",
    "from scipy.linalg import norm\n",
    "from scipy.optimize import minimize, LinearConstraint\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# function to set seeds for the imports to repeat results\n",
    "#\n",
    "\n",
    "def set_seed(tmp_seed):\n",
    "    np.random.seed(tmp_seed)\n",
    "    tf.random.set_seed(tmp_seed)\n",
    "    torch.manual_seed(tmp_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# function to creat folder for saving results\n",
    "#\n",
    "\n",
    "def create_folder(tmp_seed):\n",
    "\n",
    "    # Set the path the results should be saved in\n",
    "    SAVEPATH = R'saved_networks/alternating_training/_seed_'\n",
    "    \n",
    "    # create folder if not exist\n",
    "    if os.path.exists(os.path.normpath(SAVEPATH +str(tmp_seed))):\n",
    "        shutil.rmtree(os.path.normpath(SAVEPATH +str(tmp_seed)))\n",
    "        os.makedirs(os.path.normpath(SAVEPATH +str(tmp_seed)))\n",
    "    else:\n",
    "        os.makedirs(os.path.normpath(SAVEPATH +str(tmp_seed)))\n",
    "\n",
    "    return SAVEPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# function to prepare the training set\n",
    "#\n",
    "\n",
    "def prepare(batch_size):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    "    )\n",
    "\n",
    "    train_set = torchvision.datasets.MNIST(\n",
    "        root=\"../\", train=True, download=False, transform=transform\n",
    "    )\n",
    "\n",
    "    indices = train_set.targets==3\n",
    "    train_set.data , train_set.targets = train_set.data[indices], train_set.targets[indices]\n",
    "\n",
    "    # 6131 is len of data. with batch size 32 this is not possible, so 6131/32 = 191.59 -> 191 executions\n",
    "    train_set.data = train_set.data[:6112]\n",
    "    train_set.targets = train_set.targets[:6112]\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This cell is for drawing the image\n",
    "# if the parameter save is set, the images are also saved\n",
    "#\n",
    "\n",
    "def view_samples(samples,save,path,tmp_seed,epoch):\n",
    "    for i in range(16):\n",
    "        ax = plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(samples[i].reshape(28, 28), cmap=\"gray_r\")\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    if(save):\n",
    "        if not os.path.isdir(os.path.normpath(path +str(tmp_seed)+R'/plots')):\n",
    "            os.makedirs(os.path.normpath(path +str(tmp_seed)+R'/plots/pdf'))\n",
    "            os.makedirs(os.path.normpath(path +str(tmp_seed)+R'/plots/png'))\n",
    "            #os.makedirs(os.path.normpath(path +str(tmp_seed)+R'/plots/png/image_values'))\n",
    "            os.makedirs(os.path.normpath(path +str(tmp_seed)+R'/plots/networks'))\n",
    "        plt.savefig(os.path.normpath(path +str(tmp_seed)+R'/plots/pdf/generated_images_e%d.pdf'%epoch), bbox_inches='tight')\n",
    "        plt.savefig(os.path.normpath(path +str(tmp_seed)+R'/plots/png/generated_images_e%d.png'%epoch), bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This cell defines the discriminator network which is a network\n",
    "# input size: 784 (shape to (None,1,28,28))\n",
    "# hidden layer: 512\n",
    "# output size: 1\n",
    "#\n",
    "\n",
    "def setup_dis(lr):\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(tf.keras.layers.Reshape((784,), input_shape=(1,28,28)))\n",
    "    discriminator.add(Dense(512, activation='relu'))\n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "    org_weights_dis = discriminator.get_weights()\n",
    "    discriminator.compile(loss= tf.keras.losses.BinaryCrossentropy(), optimizer=optimizers.SGD(learning_rate=lr),metrics=[\"accuracy\"])\n",
    "    \n",
    "    return discriminator, org_weights_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This cell defines the generator network which is a network\n",
    "# input size: 10\n",
    "# hidden layer: 512\n",
    "# output size: 784 (shape to (None,1,28,28))\n",
    "#\n",
    "\n",
    "def setup_gen():\n",
    "    generator = Sequential()\n",
    "    generator.add(Dense(512, input_dim=10, activation='relu'))\n",
    "    generator.add(Dense(784, activation='tanh'))\n",
    "    generator.add(tf.keras.layers.Reshape((1, 28, 28)))\n",
    "    org_weights_gen = generator.get_weights()\n",
    "\n",
    "    return generator, org_weights_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This cell defines the GAN network which combines the Generator and the Discriminator.\n",
    "# In case we dont want to train the Discriminator when we train the Generator we have to set the discriminator.trainable to False\n",
    "#\n",
    "\n",
    "def setup_gan(discriminator, generator,lr):\n",
    "    \n",
    "    discriminator.trainable = False\n",
    "    GAN = Sequential()\n",
    "    GAN.add(generator)\n",
    "    GAN.add(discriminator)\n",
    "    GAN.compile(loss=tf.keras.losses.BinaryCrossentropy() , optimizer=optimizers.SGD(learning_rate=lr),metrics=[\"accuracy\"])\n",
    "\n",
    "    return GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# prepare training by setting parameters\n",
    "# every digit will be trained 30 times. -> 30 num_epochs * len_trainloader\n",
    "#\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "len_trainloader = 191\n",
    "number_calculations = len_trainloader*num_epochs\n",
    "\n",
    "\n",
    "# set up the label for fake or real images\n",
    "# labels = 0 for discriminator for fake images\n",
    "# labels = 1 for discriminator for real images and generator for fake images\n",
    "real_samples_labels = np.ones((batch_size, 1))\n",
    "generated_samples_labels = np.zeros((batch_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   Train function to train the GAN\n",
    "#\n",
    "\n",
    "def train(discriminator, generator, GAN, org_weights_dis,org_weights_gen, SAVEPATH, tmp_seed,train_loader,lr,alpha):\n",
    "    # setup the lists to plot the error after training\n",
    "    error_discriminator = []\n",
    "    error_generator = []\n",
    "    \n",
    "    # set the start time\n",
    "    st = time.time()\n",
    "\n",
    "    # reset weights for each training\n",
    "    generator.set_weights(org_weights_gen)\n",
    "    discriminator.set_weights(org_weights_dis)\n",
    "\n",
    "    # train the GAN\n",
    "    for epoch in range(1,number_calculations+1):\n",
    "        # use next samples of trainloader \n",
    "        real_samples, mnist_labels = next(iter(train_loader))\n",
    "\n",
    "        # Data for training the discriminator\n",
    "        latent_space_samples = torch.randn((batch_size, 10))\n",
    "\n",
    "        generated_samples = generator.predict(np.vstack(latent_space_samples),verbose=0)\n",
    "        all_samples,all_samples_labels = np.vstack((real_samples, generated_samples)),np.vstack((real_samples_labels, generated_samples_labels))\n",
    "        \n",
    "        d_loss, _ = discriminator.train_on_batch(all_samples, all_samples_labels)\n",
    "        \n",
    "        g_loss,_ = GAN.train_on_batch(np.vstack(latent_space_samples), real_samples_labels)\n",
    "     \n",
    "        # Show loss\n",
    "        if(epoch%len_trainloader==0):\n",
    "            k = epoch/len_trainloader\n",
    "            generated_samples = generator.predict(np.vstack(latent_space_samples),verbose=0)\n",
    "            all_samples,all_samples_labels = np.vstack((real_samples, generated_samples)),np.vstack((real_samples_labels, generated_samples_labels))\n",
    "            d_loss, _ = discriminator.evaluate(all_samples, all_samples_labels, verbose=0)\n",
    "            g_loss,_ = GAN.evaluate(np.vstack(latent_space_samples), real_samples_labels,verbose=0)\n",
    "\n",
    "            print(f\"Epoch: {k} Loss D.: {d_loss}\")\n",
    "            print(f\"Epoch: {k} Loss G.: {g_loss}\")\n",
    "            with open(os.path.normpath(SAVEPATH +str(tmp_seed)+'/performance.txt'), \"a+\") as outfile:\n",
    "                # summarize discriminator performance\n",
    "                outfile.write(f\"Epoch: {k} Loss D.: {d_loss}\"+\"\\n\")\n",
    "                outfile.write(f\"Epoch: {k} Loss G.: {g_loss}\"+\"\\n\")\n",
    "                \n",
    "            generated_samples = generator.predict(np.vstack(latent_space_samples),verbose=0)\n",
    "\n",
    "            view_samples(generated_samples,True,SAVEPATH,tmp_seed,k)\n",
    "\n",
    "            #decrease learning_rate\n",
    "            lr = lr*alpha\n",
    "\n",
    "            # reset trainloader\n",
    "            train_loader = prepare(batch_size)\n",
    "\n",
    "    # get the end time\n",
    "    et = time.time()\n",
    "\n",
    "    # get the execution time\n",
    "    elapsed_time = et - st\n",
    "\n",
    "    # evaluate the loss after training\n",
    "    latent_space_samples = torch.randn((batch_size, 10))\n",
    "\n",
    "    generated_samples = generator.predict(np.vstack(latent_space_samples),verbose=0)\n",
    "\n",
    "    all_samples,all_samples_labels = np.vstack((real_samples, generated_samples)),np.vstack((real_samples_labels, generated_samples_labels))\n",
    "\n",
    "    d_loss, _ = discriminator.evaluate(all_samples, all_samples_labels, verbose=0)\n",
    "    g_loss,_ = GAN.evaluate(np.vstack(latent_space_samples), real_samples_labels,verbose=0)\n",
    "\n",
    "    k = epoch/len_trainloader\n",
    "    # print performance of last run\n",
    "    print(f\"Epoch: {k} Loss D.: {d_loss}\")\n",
    "    print(f\"Epoch: {k} Loss G.: {g_loss}\")\n",
    "    with open(os.path.normpath(SAVEPATH +str(tmp_seed)+'/performance.txt'), \"a+\") as outfile:\n",
    "        # summarize discriminator performance\n",
    "        outfile.write(f\"Epoch: {k} Loss D.: {d_loss}\"+\"\\n\")\n",
    "        outfile.write(f\"Epoch: {k} Loss G.: {g_loss}\"+\"\\n\")    \n",
    "\n",
    "    # append the loss for plotting the loss for each lambda_weighted\n",
    "    error_generator.append(g_loss)\n",
    "    error_discriminator.append(d_loss)\n",
    "\n",
    "    generated_samples = generator.predict(np.vstack(latent_space_samples),verbose=0)\n",
    "\n",
    "    view_samples(generated_samples,True,SAVEPATH,tmp_seed,k)\n",
    "\n",
    "    # save the weights of the generator and discriminator\n",
    "    generator.save(os.path.normpath(SAVEPATH +str(tmp_seed)+'/plots/networks/generator.h5'))\n",
    "    discriminator.save(os.path.normpath(SAVEPATH +str(tmp_seed)+'/plots/networks/discriminator.h5'))\n",
    "        \n",
    "    return error_generator, error_discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of initializations\n",
    "number_seeds = 3\n",
    "\n",
    "\n",
    "all_errors_gen = []\n",
    "all_errors_dis = []\n",
    "start_alltime = time.time()\n",
    "\n",
    "# setup learning rate and decrease factor\n",
    "lr = 0.01\n",
    "alpha = 0.99\n",
    "\n",
    "# train GAN for each initialization\n",
    "for i in range(number_seeds):\n",
    "    # generate and set seed\n",
    "    seed = np.random.randint(99999999)\n",
    "    set_seed(seed)\n",
    "\n",
    "    # create path\n",
    "    path = create_folder(seed)\n",
    "\n",
    "    # setup images for this seed\n",
    "    train_loader = prepare(batch_size)\n",
    "\n",
    "    # setup weights for this seed\n",
    "    dis,org_dis = setup_dis(lr)\n",
    "    gen,org_gen = setup_gen()\n",
    "    gan = setup_gan(dis,gen,lr)\n",
    "\n",
    "    # train the GAN for specific initialization\n",
    "    error_generator, error_discriminator = train(dis,gen,gan, org_dis, org_gen,path,seed,train_loader,lr,alpha)\n",
    "    all_errors_gen.extend(error_generator)\n",
    "    all_errors_dis.extend(error_discriminator)\n",
    "\n",
    "# save computation time of all seeds\n",
    "end_alltime = time.time()\n",
    "\n",
    "# get the execution time\n",
    "computation_time = end_alltime - start_alltime\n",
    "time_string = '>>>>>>>Complete computatiom time: %.3f seconds<<<<<<<' %np.round(computation_time,3)\n",
    "print(time_string)\n",
    "\n",
    "# save computatuion time\n",
    "with open(os.path.normpath(path +str(seed) +'/../performance.txt'), \"a+\") as outfile:   \n",
    "    outfile.write(time_string+\"\\n\"+\"\\n\")\n",
    "\n",
    "# plot the gan errors for all seeds    \n",
    "plt.scatter([all_errors_gen], [all_errors_dis], color= 'red',s = 20,cmap='RdYlGn_r', vmin=0, vmax=3)\n",
    "plt.colorbar(label=\"image evaluation\")\n",
    "plt.title(\"GAN loss over seeds\")\n",
    "plt.xlabel('Generator_loss', fontsize=18)\n",
    "plt.ylabel('Discriminator_loss', fontsize=16)\n",
    "plt.savefig(os.path.normpath(path +str(seed)+'/../GAN_error_over_seeds.pdf')) \n",
    "plt.close()\n",
    "\n",
    "#\n",
    "# plot and save the losses for all seeds\n",
    "#\n",
    "\n",
    "plt.scatter([all_errors_gen], [all_errors_dis], color= 'red',s = 20,cmap='RdYlGn_r', vmin=0, vmax=3)\n",
    "plt.colorbar(label=\"image evaluation\")\n",
    "plt.xscale('symlog')\n",
    "plt.yscale('symlog')\n",
    "plt.title(\"GAN loss over seeds in log scale\")\n",
    "plt.xlabel('Generator_loss', fontsize=18)\n",
    "plt.ylabel('Discriminator_loss', fontsize=16)\n",
    "plt.savefig(os.path.normpath(path +str(seed)+'/../GAN_error_over_seeds_logscale.pdf'))\n",
    "plt.close()\n",
    "\n",
    "#\n",
    "# save the erros into csv file to reuse them\n",
    "#\n",
    "\n",
    "with open(os.path.normpath(path +str(seed)+'/../saved_errors.csv'), 'w') as f:\n",
    "    # using csv.writer method from CSV package\n",
    "    write = csv.writer(f)\n",
    "    write.writerows([all_errors_gen])\n",
    "    write.writerows([all_errors_dis])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e11b55361148b6c3db81e2c737cd36da1a961dccb25eb888a6a7c970ac9dd9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
