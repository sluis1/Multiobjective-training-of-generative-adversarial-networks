{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This is an example for a GAN for a 3x3 image construction\n",
    "# The optimization is done with the accelerated multiobjective gradient method\n",
    "# Factor here is set to 100 instead of 2\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils.layer_utils import count_params  \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers, losses\n",
    "import csv\n",
    "\n",
    "from scipy.linalg import norm\n",
    "from scipy.optimize import minimize, LinearConstraint\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# function to set seeds for the imports to repeat results\n",
    "#\n",
    "\n",
    "def set_seed(tmp_seed):\n",
    "    np.random.seed(tmp_seed)\n",
    "    tf.random.set_seed(tmp_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# function to creat folder for saving results\n",
    "#\n",
    "\n",
    "def create_folder(tmp_seed):\n",
    "\n",
    "    # Set the path the results should be saved in\n",
    "    SAVEPATH = R'saved_networks/Kons1_E/_seed_'\n",
    "    \n",
    "    # create folder if not exist\n",
    "    if os.path.exists(os.path.normpath(SAVEPATH +str(tmp_seed))):\n",
    "        shutil.rmtree(os.path.normpath(SAVEPATH +str(tmp_seed)))\n",
    "        os.makedirs(os.path.normpath(SAVEPATH +str(tmp_seed)))\n",
    "    else:\n",
    "        os.makedirs(os.path.normpath(SAVEPATH +str(tmp_seed)))\n",
    "\n",
    "    return SAVEPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This cell is for drawing the 3x3 image\n",
    "# if the parameter save is set, the images are also saved\n",
    "#\n",
    "\n",
    "def view_samples(samples, m, n,save,path,tmp_seed):\n",
    "    fig, axes = plt.subplots(figsize=(10, 10), nrows=m, ncols=n, sharey=True, sharex=True)\n",
    "    for ax, img in zip(axes.flatten(), samples):\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        im = ax.imshow(1-img.reshape((3,3)), cmap='Greys_r')\n",
    "    if(save):\n",
    "        if not os.path.isdir(os.path.normpath(path +str(tmp_seed)+R'/plots')):\n",
    "            os.makedirs(os.path.normpath(path +str(tmp_seed)+R'/plots/pdf'))\n",
    "            os.makedirs(os.path.normpath(path +str(tmp_seed)+R'/plots/png'))\n",
    "            os.makedirs(os.path.normpath(path +str(tmp_seed)+R'/plots/png/image_values'))\n",
    "            os.makedirs(os.path.normpath(path +str(tmp_seed)+R'/plots/networks'))\n",
    "        plt.savefig(os.path.normpath(path +str(tmp_seed)+R'/plots/pdf/generated_images.pdf'), bbox_inches='tight')\n",
    "        plt.savefig(os.path.normpath(path +str(tmp_seed)+R'/plots/png/generated_images.png'), bbox_inches='tight')\n",
    "    plt.close()\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This cell sets up the real images\n",
    "# Noise is added to a standard image\n",
    "#\n",
    "\n",
    "def initialize_images():\n",
    "    num_images = 50\n",
    "    faces = []\n",
    "    standard_face = [0.9,0.1,0.1,0.1,0.9,0.1,0.1,0.1,0.9]\n",
    "\n",
    "    for i in range(num_images):\n",
    "        noise = np.random.normal(0,0.05,9)\n",
    "        noise_face = standard_face + noise\n",
    "        faces.append(noise_face)\n",
    "    faces = np.array(faces)\n",
    "\n",
    "    _ = view_samples(faces, 1, 8,False,None,None)\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This cell defines the discriminator network which is a network\n",
    "# input size: 9\n",
    "# output size: 1\n",
    "#\n",
    "\n",
    "def setup_dis():\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Dense(1, input_dim=9, activation='sigmoid'))\n",
    "    discriminator.compile(loss=\"binary_crossentropy\", optimizer=optimizers.SGD(learning_rate=0.1),metrics=[\"accuracy\"])\n",
    "\n",
    "    # save the startweights for reset the weigths after training\n",
    "    org_weights_dis = discriminator.get_weights()\n",
    "    \n",
    "    return discriminator, org_weights_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This cell defines the generator network which is a network\n",
    "# input size: 1\n",
    "# output size: 9\n",
    "#\n",
    "\n",
    "def setup_gen():\n",
    "    generator = Sequential()\n",
    "    generator.add(Dense(9, input_dim=1, activation='sigmoid'))\n",
    "    \n",
    "    # save the startweights for reset the weigths after training\n",
    "    org_weights_gen = generator.get_weights()\n",
    "\n",
    "    return generator, org_weights_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This cell defines the GAN network which combines the Generator and the Discriminator.\n",
    "# In case we dont want to train the Discriminator when we train the Generator we have to set the discriminator.trainable to False\n",
    "#\n",
    "\n",
    "def setup_gan(discriminator, generator):\n",
    "    \n",
    "    discriminator.trainable = False\n",
    "    GAN = Sequential()\n",
    "    GAN.add(generator)\n",
    "    GAN.add(discriminator)\n",
    "    GAN.compile(loss=\"binary_crossentropy\", optimizer=optimizers.SGD(learning_rate=0.1),metrics=[\"accuracy\"])\n",
    "\n",
    "    return GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This is the function we want to minimize which is || h*( sum(xi * nabla fi(y^k))- ((k-1)/(k+2)*(x^k - x^(k-1)))||^2\n",
    "#\n",
    "\n",
    "def f(x,params):\n",
    "    A, fac, h = params\n",
    "    return norm((h*(x[0]*A[0,:] + x[1] * A[1,:])) - fac )**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This is the function which call minimize function with the constraints.\n",
    "# The constraints are xi>0 and sum(xi) = 1 \n",
    "#\n",
    "\n",
    "def minimization(A, fac, h):\n",
    "    # this is the initial guess, here set to 1\n",
    "    initial_guess = np.ones(2)\n",
    "\n",
    "    # this is a Matrix [[1,0],[0,1]]\n",
    "    B = np.zeros((2,2))\n",
    "    B[0,0] = 1\n",
    "    B[1,1] = 1\n",
    "\n",
    "    # the lower bound verifies that xi >0 \n",
    "    lb = np.array([0,0])\n",
    "\n",
    "    # one constraint is for xi>0 and one for sum(xi) = 1\n",
    "    const = [LinearConstraint(B,lb,np.inf), {'type':'eq','fun': lambda x:  np.sum(x)-1.0}]\n",
    "\n",
    "    # minimize the function f with A factor and h\n",
    "    res = minimize(f, initial_guess, method = 'SLSQP' , args= [A, fac, h], constraints = const, options={'ftol': 1e-6,'disp': False}) # here true for display\n",
    "\n",
    "    # return the result\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This function calculates the gradients for the minimization problem\n",
    "# For the jacobian Matrix here the off diagonal is set the derivative of the error\n",
    "# The result is A = [[ED_D,ED_G],[EG_D,EG_G]]\n",
    "#\n",
    "\n",
    "def calculate_gradients(gan_model,d_model, z, x_real, y_real,y_fake,batch_size):\n",
    "    # set the loss function for the GAN model to BinaryCrossentropy()\n",
    "    bce = losses.BinaryCrossentropy()\n",
    "\n",
    "    EG_wG = []\n",
    "    # Calculate gradients with respect to every trainable variable for EG_G\n",
    "    with tf.GradientTape() as tape:\n",
    "        gan_pred = gan_model(z)\n",
    "        loss = bce(y_real, gan_pred)\n",
    "        EG_G = tape.gradient(loss, gan_model.trainable_weights)\n",
    "    for i in range(len(EG_G)):\n",
    "        EG_wG.extend(EG_G[i].numpy().reshape(-1))\n",
    "\n",
    "\n",
    "    # Calculate gradients with respect to every trainable variable for ED_G. Here D(X)/G = 0 \n",
    "    ED_wG = [] \n",
    "    with tf.GradientTape() as tape:\n",
    "        gan_pred = gan_model(z)\n",
    "        loss = bce(y_fake, gan_pred)\n",
    "        ED_G = tape.gradient(loss, gan_model.trainable_weights)\n",
    "    for i in range(len(ED_G)):\n",
    "        ED_wG.extend(ED_G[i].numpy().reshape(-1))\n",
    "\n",
    "    # Calculate gradients with respect to every trainable variable for EG_D\n",
    "    d_model.trainable = True\n",
    "    EG_wD = []\n",
    "    # Calculate gradients with respect to every trainable variable for EG_G\n",
    "    with tf.GradientTape() as tape:\n",
    "        gan_pred = gan_model(z)\n",
    "        loss = bce(y_real, gan_pred)\n",
    "        EG_D = tape.gradient(loss, d_model.trainable_weights)\n",
    "    for i in range(len(EG_D)):\n",
    "        EG_wD.extend(EG_D[i].numpy().reshape(-1))\n",
    "    \n",
    "    # Calculate gradients with respect to every trainable variable for ED_D\n",
    "    # must set the trainable weights to True to get results because otherwise the number of weights is 0\n",
    "    ED_wD = [] \n",
    "    with tf.GradientTape() as tape:\n",
    "        gan_pred = gan_model(z)\n",
    "        dis_pred = d_model(x_real)\n",
    "\n",
    "        # here use batch:size if it is available\n",
    "        X,Y = tf.reshape(tf.stack((dis_pred,gan_pred)),shape=(2*batch_size,1)), tf.reshape(tf.stack((y_real,y_fake)),shape=(2*batch_size,1))\n",
    "        loss = bce(Y,X)\n",
    "        ED_D = tape.gradient(loss, d_model.trainable_weights)\n",
    "    for i in range(len(ED_D)):\n",
    "        ED_wD.extend(ED_D[i].numpy().reshape(-1))\t\t\t\n",
    "    d_model.trainable = False\n",
    "\n",
    "    # set the first row the Error of the Discriminator [ED_wD,0]\n",
    "    ED = []\n",
    "    for grad in [ED_wD,ED_wG]:\n",
    "        ED.extend(grad)\n",
    "\n",
    "    # set the second row the Error of the Generator [0,ED_wG]\n",
    "    EG = []\n",
    "    for grad in [EG_wD,EG_wG]:\n",
    "        EG.extend(grad)\n",
    "\n",
    "    # calculate jacobian matrix\n",
    "    return np.array([ED,EG])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of real and fake images to train in epoch\n",
    "batch_size = 8\n",
    "\n",
    "# set up the label for fake or real images\n",
    "# labels = 0 for discriminator for fake images\n",
    "# labels = 1 for discriminator for real images and generator for fake images\n",
    "Y_fake = np.zeros((batch_size, 1))\n",
    "Y_real = np.ones((batch_size, 1))\n",
    "\n",
    "# define the learning rate.\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   This function generates the evaluation/color of the generated images\n",
    "#   use sum of difference between real and fake images for each pixel\n",
    "#\n",
    "\n",
    "def calculate_color(generated_images):\n",
    "    best_image = [0.9,0.1,0.1,0.1,0.9,0.1,0.1,0.1,0.9]\n",
    "    norms = []\n",
    "    for image in generated_images:\n",
    "        image = np.array(image[0])\n",
    "        diff = best_image-image\n",
    "        norms.append(sum(abs(diff))) # Manhattan norm\n",
    "\n",
    "    avg_norm = np.mean(norms)\n",
    "    return avg_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   Train function to train the GAN\n",
    "#\n",
    "\n",
    "def train(discriminator, generator, GAN, org_weights_dis,org_weights_gen, SAVEPATH, tmp_seed,faces):\n",
    "    \n",
    "    # setupt the lists to plot the error after training\n",
    "    error_discriminator = []\n",
    "    error_generator = []\n",
    "\n",
    "    # initialize colorlist\n",
    "    new_color = []\n",
    "    \n",
    "    # set the start time\n",
    "    st = time.time()\n",
    "\n",
    "    # reset weights for each training\n",
    "    generator.set_weights(org_weights_gen)\n",
    "    discriminator.set_weights(org_weights_dis)\n",
    "\n",
    "    # set the at first x^(k-1) to x^k which are the actual weights\n",
    "    x_gk_m1 = generator.get_weights()\n",
    "    x_dk_m1 = discriminator.get_weights()\n",
    "\n",
    "    # set k to 1\n",
    "    k = 1\n",
    "\n",
    "    # process the minimization until norm(v) < 1e-4 or we have reached 1000 minimization steps in case this is also done for alternating training\n",
    "    while (True):\n",
    "        # generate batch_size many inputs and fake images\n",
    "        Z = np.random.uniform(size=batch_size)\n",
    "        # choose batch_size many real images from the pool of real images\n",
    "        X_real = np.random.permutation(faces)[:batch_size]\n",
    "            \n",
    "        # set x_k for the generator and the discriminator\n",
    "        x_gk = generator.get_weights()\n",
    "        x_dk = discriminator.get_weights()\n",
    "\n",
    "\n",
    "        # y^k = x^k + ((k-1)/(k+100)*(x^k-x^(k-1))) fpr discriminator and generator\n",
    "        # for the discriminator\n",
    "        # need first save as tmp because change directly the weights\n",
    "        x_dk_tmp = discriminator.get_weights()\n",
    "        x_gk_tmp = generator.get_weights()\n",
    "\n",
    "        for i in range(len(x_dk_tmp)):\n",
    "            x_dk_tmp[i] = x_dk_tmp[i] + ((k-1)/(k+1000)*(x_dk_tmp[i]-x_dk_m1[i])) \n",
    "        discriminator.set_weights(x_dk_tmp)\n",
    "        # for the generator\n",
    "        for i in range(len(x_gk_tmp)):\n",
    "            x_gk_tmp[i] = x_gk_tmp[i] + ((k-1)/(k+1000)*(x_gk_tmp[i]-x_gk_m1[i]))\n",
    "        generator.set_weights(x_gk_tmp)\n",
    "\n",
    "        # calculate jacobian matrix\n",
    "        A = calculate_gradients(GAN,discriminator, Z, X_real, Y_real,Y_fake,batch_size)\n",
    "\n",
    "        # calculate factor = (((k-1)/(k+100))*(x_k-x_k_m1))) for generator and discriminator\n",
    "        # this factor is used in minimization problem\n",
    "        factor = []\n",
    "        for i in range(len(x_dk)):\n",
    "            factor.extend((k-1)/(k+1000)*(x_dk[i].reshape(-1)-x_dk_m1[i].reshape(-1)))\n",
    "        for i in range(len(x_gk)):\n",
    "            factor.extend((k-1)/(k+1000)*(x_gk[i].reshape(-1)-x_gk_m1[i].reshape(-1)))\n",
    "\n",
    "        # solve the minimization problem\n",
    "        theta = minimization(A, factor, learning_rate)\n",
    "\n",
    "        # now get the v to update the weights by calculate the sum of theta*A\n",
    "        v = theta.x[0]*A[0,:] + theta.x[1]*A[1,:]\n",
    "\n",
    "        # split v into v_discriminator and v_generator to update the weights for each network\n",
    "        # in case that A is first row is ED and second row is EG first v is for weights of the discriminator\n",
    "        discriminator.trainable = True\n",
    "        v_dis = v[:count_params(discriminator.trainable_weights)]\n",
    "        v_gen = v[count_params(discriminator.trainable_weights):]\n",
    "        discriminator.trainable = False\n",
    "\n",
    "\n",
    "        # set x_k_m1 to x_k\n",
    "        # this is for the next iteration to have x^(k-1) = x^k in the next iteration\n",
    "        x_gk_m1 = x_gk\n",
    "        x_dk_m1 = x_dk\n",
    "\n",
    "        # update the weights for the the discriminator and generator\n",
    "        wd = discriminator.get_weights()\n",
    "        for i in range(len(wd)):\n",
    "            wd[i] = wd[i] - (learning_rate* v_dis[:wd[i].size]).reshape(wd[i].shape)\n",
    "            v_dis = v_dis[wd[i].size:]\n",
    "        discriminator.set_weights(wd)\n",
    "        \n",
    "        wg = generator.get_weights()\n",
    "        for i in range(len(wg)):\n",
    "            wg[i] = wg[i] - (learning_rate* v_gen[:wg[i].size]).reshape(wg[i].shape)\n",
    "            v_gen = v_gen[wg[i].size:]\n",
    "        generator.set_weights(wg)\n",
    "\n",
    "        # if the norm is smaller than 1e-4 or the number of iterations greater than 1000 stop\n",
    "        if (norm(v) < 1e-4 or k >=1000):\n",
    "            break\n",
    "\n",
    "        # increase k\n",
    "        k +=1\n",
    "\n",
    "        # print the loss every 100 iterations\n",
    "        if((k)%100 == 0):\n",
    "\n",
    "            # calculate loss for plotting after 100 iterations\n",
    "            # combine fake and real images\n",
    "            X_fake = generator.predict(Z,verbose = 0)\n",
    "            X, Y = np.vstack((X_real, X_fake)), np.vstack((Y_real, Y_fake))\n",
    "            d_loss, _ = discriminator.evaluate(X, Y, verbose=0)\n",
    "\n",
    "            g_loss,_ = GAN.evaluate(Z, Y_real,verbose=0)\n",
    "\n",
    "            loss_string = '>>Loss iteration:%d, d=%.4f, g=%.4f' % (k, d_loss, g_loss)\n",
    "\n",
    "            print(loss_string)\n",
    "            \n",
    "            # save the loss ever 100 iteration\n",
    "            with open(os.path.normpath(SAVEPATH +str(tmp_seed)+'/performance.txt'), \"a+\") as outfile:\n",
    "                # summarize discriminator performance\n",
    "                outfile.write(loss_string+\"\\n\")\n",
    "\n",
    "    # generate new images to evaluate the model after training\n",
    "    Z = np.random.uniform(size=batch_size)\n",
    "    X_fake = generator.predict(Z,verbose = 0)\n",
    "    X_real = np.random.permutation(faces)[:batch_size]\n",
    "    X, Y = np.vstack((X_real, X_fake)), np.vstack((Y_real, Y_fake))\n",
    "\n",
    "    d_loss, _ = discriminator.evaluate(X, Y, verbose=0)\n",
    "    g_loss,_ = GAN.evaluate(Z, Y_real,verbose=0)\n",
    "\n",
    "    # save last iteration\n",
    "    loss_string = '>>Loss iteration:%d, d=%.4f, g=%.4f' % (k, d_loss, g_loss)\n",
    "\n",
    "    print(loss_string)\n",
    "    # save the loss ever 100 iteration\n",
    "    with open(os.path.normpath(SAVEPATH +str(tmp_seed)+'/performance.txt'), \"a+\") as outfile:\n",
    "        # summarize discriminator performance\n",
    "        outfile.write(loss_string+\"\\n\")        \n",
    "        \n",
    "    # evaluate the discriminator after training\n",
    "    # evaluate discriminator on real examples\n",
    "    _, acc_real = discriminator.evaluate(np.vstack(X_real),Y_real, verbose=0)\n",
    "    # evaluate discriminator on fake examples\n",
    "    _, acc_fake = discriminator.evaluate(np.vstack(X_fake),Y_fake, verbose=0)\n",
    "\n",
    "    # get the end time\n",
    "    et = time.time()\n",
    "\n",
    "    # get the execution time\n",
    "    elapsed_time = et - st\n",
    "    \n",
    "    # print accuracy and time\n",
    "    acc_string = '>Accuracy discriminator: real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100)\n",
    "    time_string = '>>>>>>>Execution time: %.3f seconds<<<<<<<' %np.round(elapsed_time,3)\n",
    "    print(acc_string)\n",
    "    print(time_string)\n",
    "\n",
    "    # save accuracy and time \n",
    "    with open(os.path.normpath(SAVEPATH +str(tmp_seed)+'/performance.txt'), \"a+\") as outfile:        \n",
    "        # Calculate the discriminator accuracy\n",
    "        outfile.write(acc_string+\"\\n\")\n",
    "        # save the acutal norm of v\n",
    "        outfile.write(time_string+\"\\n\"+\"\\n\")\n",
    "        \n",
    "    # append the loss for plotting the loss for each lambda_weighted\n",
    "    error_generator.append(g_loss)\n",
    "    error_discriminator.append(d_loss)\n",
    "\n",
    "    # generate images\n",
    "    generated_images = []\n",
    "    for i in range(4):\n",
    "        z = np.random.randn(1,1)\n",
    "        z = [z]\n",
    "        generated_image = generator.predict(z,verbose=0)\n",
    "        generated_images.append(generated_image)\n",
    "    _ = view_samples(generated_images, 1, 4,True,SAVEPATH,tmp_seed)\n",
    "\n",
    "    # claculate the color for the generated images\n",
    "    new_color.append(calculate_color(generated_images))\n",
    "\n",
    "    # save the values of the generated images\n",
    "    for i in generated_images:\n",
    "        with open(os.path.normpath(SAVEPATH +str(tmp_seed)+R'/plots/png/image_values/generated_images.txt'), \"a+\") as outfile:\n",
    "            # summarize discriminator performance\n",
    "            outfile.write(str(i)+\"\\n\")\n",
    "\n",
    "    # save the weights of the generator and discriminator\n",
    "    generator.save(os.path.normpath(SAVEPATH +str(tmp_seed)+'/plots/networks/generator.h5'))\n",
    "    discriminator.save(os.path.normpath(SAVEPATH +str(tmp_seed)+'/plots/networks/discriminator.h5'))\n",
    "        \n",
    "    return error_generator, error_discriminator, new_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of initializations\n",
    "number_seeds = 10\n",
    "\n",
    "all_errors_gen = []\n",
    "all_errors_dis = []\n",
    "all_colors = []\n",
    "start_alltime = time.time()\n",
    "\n",
    "# train GAN for each initialization\n",
    "for i in range(number_seeds):\n",
    "    # generate and set seed\n",
    "    seed = np.random.randint(99999999)\n",
    "    set_seed(seed)\n",
    "\n",
    "    # create path\n",
    "    path = create_folder(seed)\n",
    "\n",
    "    # setup images for this seed\n",
    "    faces = initialize_images()\n",
    "\n",
    "    # setup weights for this seed\n",
    "    dis,org_dis = setup_dis()\n",
    "    gen,org_gen = setup_gen()\n",
    "    gan = setup_gan(dis,gen)\n",
    "\n",
    "    # train the GAN for specific initialization\n",
    "    error_generator, error_discriminator, new_color = train(dis,gen,gan, org_dis, org_gen,path,seed,faces)\n",
    "    all_errors_gen.extend(error_generator)\n",
    "    all_errors_dis.extend(error_discriminator)\n",
    "    all_colors.extend(new_color)\n",
    "\n",
    "# save computation time of all seeds\n",
    "end_alltime = time.time()\n",
    "# get the execution time\n",
    "computation_time = end_alltime - start_alltime\n",
    "time_string = '>>>>>>>Complete computatiom time: %.3f seconds<<<<<<<' %np.round(computation_time,3)\n",
    "print(time_string)\n",
    "\n",
    "# save number of good and bad images\n",
    "n_green = 0\n",
    "n_yellow = 0\n",
    "n_red = 0\n",
    "for val in all_colors:\n",
    "    if(val<=1):\n",
    "        n_green +=1\n",
    "    elif(val<=2):\n",
    "        n_yellow +=1\n",
    "    else:\n",
    "        n_red+=1\n",
    "\n",
    "# save computatuion time and number images\n",
    "with open(os.path.normpath(path +str(seed) +'/../performance.txt'), \"a+\") as outfile:        \n",
    "\n",
    "    outfile.write(\"Number of great images:\"+str(n_green)+\"\\n\")\n",
    "    outfile.write(\"Number of good images:\"+str(n_yellow)+\"\\n\")\n",
    "    outfile.write(\"Number of bad images:\"+str(n_red)+\"\\n\")\n",
    "    outfile.write(time_string+\"\\n\"+\"\\n\")\n",
    "\n",
    "# plot the gan errors for all seeds    \n",
    "plt.scatter([all_errors_gen], [all_errors_dis], c= all_colors,s = 20,cmap='RdYlGn_r', vmin=0, vmax=3)\n",
    "plt.colorbar(label=\"image evaluation\")\n",
    "plt.title(\"GAN loss over seeds\")\n",
    "plt.xlabel('Generator_loss', fontsize=18)\n",
    "plt.ylabel('Discriminator_loss', fontsize=16)\n",
    "plt.savefig(os.path.normpath(path +str(seed)+'/../GAN_error_over_seeds.pdf')) \n",
    "plt.close()\n",
    "\n",
    "#\n",
    "# plot and save the losses for all seeds\n",
    "#\n",
    "\n",
    "plt.scatter([all_errors_gen], [all_errors_dis], c= all_colors,s = 20,cmap='RdYlGn_r', vmin=0, vmax=3)\n",
    "plt.colorbar(label=\"image evaluation\")\n",
    "plt.xscale('symlog')\n",
    "plt.yscale('symlog')\n",
    "plt.title(\"GAN loss over seeds in log scale\")\n",
    "plt.xlabel('Generator_loss', fontsize=18)\n",
    "plt.ylabel('Discriminator_loss', fontsize=16)\n",
    "plt.savefig(os.path.normpath(path +str(seed)+'/../GAN_error_over_seeds_logscale.pdf'))\n",
    "plt.close()\n",
    "\n",
    "#\n",
    "# save the erros into csv file to reuse them\n",
    "#\n",
    "\n",
    "with open(os.path.normpath(path +str(seed)+'/../saved_errors.csv'), 'w') as f:\n",
    "    # using csv.writer method from CSV package\n",
    "    write = csv.writer(f)\n",
    "    write.writerows([all_errors_gen])\n",
    "    write.writerows([all_errors_dis])\n",
    "    write.writerows([all_colors])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e11b55361148b6c3db81e2c737cd36da1a961dccb25eb888a6a7c970ac9dd9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
