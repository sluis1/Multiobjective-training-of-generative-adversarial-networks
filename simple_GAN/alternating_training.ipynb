{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This is an example for a GAN for a 3x3 image construction\n",
    "# The optimization is done with the alternating (normal) training\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "import csv\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# function to set seeds for the imports to repeat results\n",
    "#\n",
    "\n",
    "def set_seed(tmp_seed):\n",
    "    np.random.seed(tmp_seed)\n",
    "    tf.random.set_seed(tmp_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# function to creat folder for saving results\n",
    "#\n",
    "\n",
    "def create_folder(tmp_seed):\n",
    "\n",
    "    # Set the path the results should be saved in\n",
    "    SAVEPATH = R'saved_networks/alternating_train/_seed_'\n",
    "    \n",
    "    # create folder if not exist\n",
    "    if os.path.exists(os.path.normpath(SAVEPATH +str(tmp_seed))):\n",
    "        shutil.rmtree(os.path.normpath(SAVEPATH +str(tmp_seed)))\n",
    "        os.makedirs(os.path.normpath(SAVEPATH +str(tmp_seed)))\n",
    "    else:\n",
    "        os.makedirs(os.path.normpath(SAVEPATH +str(tmp_seed)))\n",
    "\n",
    "    return SAVEPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This cell is for drawing the 3x3 image\n",
    "# if the parameter save is set, the images are also saved\n",
    "#\n",
    "\n",
    "def view_samples(samples, m, n,save,path,tmp_seed):\n",
    "    fig, axes = plt.subplots(figsize=(10, 10), nrows=m, ncols=n, sharey=True, sharex=True)\n",
    "    for ax, img in zip(axes.flatten(), samples):\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        im = ax.imshow(1-img.reshape((3,3)), cmap='Greys_r')\n",
    "    if(save):\n",
    "        if not os.path.isdir(os.path.normpath(path +str(tmp_seed)+R'/plots')):\n",
    "            os.makedirs(os.path.normpath(path +str(tmp_seed)+R'/plots/pdf'))\n",
    "            os.makedirs(os.path.normpath(path +str(tmp_seed)+R'/plots/png'))\n",
    "            os.makedirs(os.path.normpath(path +str(tmp_seed)+R'/plots/png/image_values'))\n",
    "            os.makedirs(os.path.normpath(path +str(tmp_seed)+R'/plots/networks'))\n",
    "        plt.savefig(os.path.normpath(path +str(tmp_seed)+R'/plots/pdf/generated_images.pdf'), bbox_inches='tight')\n",
    "        plt.savefig(os.path.normpath(path +str(tmp_seed)+R'/plots/png/generated_images.png'), bbox_inches='tight')\n",
    "    plt.close()\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This cell sets up the real images\n",
    "# Noise is added to a standard image\n",
    "#\n",
    "\n",
    "def initialize_images():\n",
    "    num_images = 50\n",
    "    faces = []\n",
    "    standard_face = [0.9,0.1,0.1,0.1,0.9,0.1,0.1,0.1,0.9]\n",
    "\n",
    "    for i in range(num_images):\n",
    "        noise = np.random.normal(0,0.05,9)\n",
    "        noise_face = standard_face + noise\n",
    "        faces.append(noise_face)\n",
    "    faces = np.array(faces)\n",
    "\n",
    "    _ = view_samples(faces, 1, 8,False,None,None)\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This cell defines the discriminator network which is a network\n",
    "# input size: 9\n",
    "# output size: 1\n",
    "#\n",
    "\n",
    "def setup_dis():\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Dense(1, input_dim=9, activation='sigmoid'))\n",
    "    discriminator.compile(loss=\"binary_crossentropy\", optimizer=optimizers.SGD(learning_rate=0.1),metrics=[\"accuracy\"])\n",
    "\n",
    "    # save the startweights for reset the weigths after training\n",
    "    org_weights_dis = discriminator.get_weights()\n",
    "    \n",
    "    return discriminator, org_weights_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This cell defines the generator network which is a network\n",
    "# input size: 1\n",
    "# output size: 9\n",
    "#\n",
    "\n",
    "def setup_gen():\n",
    "    generator = Sequential()\n",
    "    generator.add(Dense(9, input_dim=1, activation='sigmoid'))\n",
    "    \n",
    "    # save the startweights for reset the weigths after training\n",
    "    org_weights_gen = generator.get_weights()\n",
    "\n",
    "    return generator, org_weights_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This cell defines the GAN network which combines the Generator and the Discriminator.\n",
    "# In case we dont want to train the Discriminator when we train the Generator we have to set the discriminator.trainable to False\n",
    "#\n",
    "\n",
    "def setup_gan(discriminator, generator):\n",
    "    \n",
    "    discriminator.trainable = False\n",
    "    GAN = Sequential()\n",
    "    GAN.add(generator)\n",
    "    GAN.add(discriminator)\n",
    "    GAN.compile(loss=\"binary_crossentropy\", optimizer=optimizers.SGD(learning_rate=0.1),metrics=[\"accuracy\"])\n",
    "\n",
    "    return GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Set the number of real and fake images to train in epoch\n",
    "batch_size = 8\n",
    "\n",
    "# set up the label for fake or real images\n",
    "# labels = 0 for discriminator for fake images\n",
    "# labels = 1 for discriminator for real images and generator for fake images\n",
    "Y_fake = np.zeros((batch_size, 1))\n",
    "Y_real = np.ones((batch_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   This function generates the evaluation/color of the generated images\n",
    "#   use sum of difference between real and fake images for each pixel\n",
    "#\n",
    "\n",
    "def calculate_color(generated_images):\n",
    "    best_image = [0.9,0.1,0.1,0.1,0.9,0.1,0.1,0.1,0.9]\n",
    "    norms = []\n",
    "    for image in generated_images:\n",
    "        image = np.array(image[0])\n",
    "        diff = best_image-image\n",
    "        norms.append(sum(abs(diff))) # Manhattan norm\n",
    "\n",
    "    avg_norm = np.mean(norms)\n",
    "    return avg_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   Train function to train the GAN\n",
    "#\n",
    "\n",
    "def train(discriminator, generator, GAN, org_weights_dis,org_weights_gen, SAVEPATH, tmp_seed,faces):\n",
    "\n",
    "    # setupt the lists to plot the error after training\n",
    "    error_discriminator = []\n",
    "    error_generator = []\n",
    "\n",
    "    # initialize colorlist\n",
    "    new_color = []\n",
    "    \n",
    "    # set the start time\n",
    "    st = time.time()\n",
    "\n",
    "    # reset weights for each training\n",
    "    generator.set_weights(org_weights_gen)\n",
    "    discriminator.set_weights(org_weights_dis)\n",
    "\n",
    "    # process the training for number of epochs\n",
    "    for k in range(epochs):\n",
    "        # generate batch_size many inputs and fake images\n",
    "        Z = np.random.uniform(size=batch_size)\n",
    "        X_fake = generator.predict(Z,verbose=0)\n",
    "\n",
    "        # choose batch_size many real images from the pool of real images\n",
    "        X_real = np.random.permutation(faces)[:batch_size]\n",
    "            \n",
    "        # combine fake and real images for training the Discriminator\n",
    "        X, Y = np.vstack((X_real, X_fake)), np.vstack((Y_real, Y_fake))\n",
    "\n",
    "        # Train the generator for k steps\n",
    "        for i in range(1):\n",
    "                d_loss,_ = discriminator.train_on_batch(X, Y)\n",
    "\n",
    "        # train the generator for 1 step and add the error to the list\n",
    "        g_loss,_ = GAN.train_on_batch(Z, Y_real)\n",
    "\n",
    "        # print the loss every 100 iterations\n",
    "        if((k)%100 == 0):\n",
    "\n",
    "            # calculate loss for plotting after 100 iterations\n",
    "            # combine fake and real images\n",
    "            X_fake = generator.predict(Z,verbose = 0)\n",
    "            X, Y = np.vstack((X_real, X_fake)), np.vstack((Y_real, Y_fake))\n",
    "            d_loss, _ = discriminator.evaluate(X, Y, verbose=0)\n",
    "\n",
    "            g_loss,_ = GAN.evaluate(Z, Y_real,verbose=0)\n",
    "\n",
    "            loss_string = '>>Loss iteration:%d, d=%.4f, g=%.4f' % (k, d_loss, g_loss)\n",
    "\n",
    "            print(loss_string)\n",
    "            \n",
    "            # save the loss ever 100 iteration\n",
    "            with open(os.path.normpath(SAVEPATH +str(tmp_seed)+'/performance.txt'), \"a+\") as outfile:\n",
    "                # summarize discriminator performance\n",
    "                outfile.write(loss_string+\"\\n\")\n",
    "\n",
    "    # generate new images to evaluate the model after training\n",
    "    Z = np.random.uniform(size=batch_size)\n",
    "    X_fake = generator.predict(Z,verbose = 0)\n",
    "    X_real = np.random.permutation(faces)[:batch_size]\n",
    "    X, Y = np.vstack((X_real, X_fake)), np.vstack((Y_real, Y_fake))\n",
    "\n",
    "    d_loss, _ = discriminator.evaluate(X, Y, verbose=0)\n",
    "    g_loss,_ = GAN.evaluate(Z, Y_real,verbose=0)\n",
    "\n",
    "    # save last iteration\n",
    "    loss_string = '>>Loss iteration:%d, d=%.4f, g=%.4f' % (k, d_loss, g_loss)\n",
    "\n",
    "    print(loss_string)\n",
    "    # save the loss ever 100 iteration\n",
    "    with open(os.path.normpath(SAVEPATH +str(tmp_seed)+'/performance.txt'), \"a+\") as outfile:\n",
    "        # summarize discriminator performance\n",
    "        outfile.write(loss_string+\"\\n\")        \n",
    "        \n",
    "    # evaluate the discriminator after training\n",
    "    # evaluate discriminator on real examples\n",
    "    _, acc_real = discriminator.evaluate(np.vstack(X_real),Y_real, verbose=0)\n",
    "    # evaluate discriminator on fake examples\n",
    "    _, acc_fake = discriminator.evaluate(np.vstack(X_fake),Y_fake, verbose=0)\n",
    "\n",
    "    # get the end time\n",
    "    et = time.time()\n",
    "\n",
    "    # get the execution time\n",
    "    elapsed_time = et - st\n",
    "    \n",
    "    # print accuracy and time\n",
    "    acc_string = '>Accuracy discriminator: real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100)\n",
    "    time_string = '>>>>>>>Execution time: %.3f seconds<<<<<<<' %np.round(elapsed_time,3)\n",
    "    print(acc_string)\n",
    "    print(time_string)\n",
    "\n",
    "    # save accuracy and time \n",
    "    with open(os.path.normpath(SAVEPATH +str(tmp_seed)+'/performance.txt'), \"a+\") as outfile:        \n",
    "        # Calculate the discriminator accuracy\n",
    "        outfile.write(acc_string+\"\\n\")\n",
    "        # save the acutal norm of v\n",
    "        outfile.write(time_string+\"\\n\"+\"\\n\")\n",
    "        \n",
    "    # append the loss for plotting the loss for each lambda_weighted\n",
    "    error_generator.append(g_loss)\n",
    "    error_discriminator.append(d_loss)\n",
    "\n",
    "    # generate images\n",
    "    generated_images = []\n",
    "    for i in range(4):\n",
    "        z = np.random.randn(1,1)\n",
    "        z = [z]\n",
    "        generated_image = generator.predict(z,verbose=0)\n",
    "        generated_images.append(generated_image)\n",
    "    _ = view_samples(generated_images, 1, 4,True,SAVEPATH,tmp_seed)\n",
    "\n",
    "    # claculate the color for the generated images\n",
    "    new_color.append(calculate_color(generated_images))\n",
    "\n",
    "    # save the values of the generated images\n",
    "    for i in generated_images:\n",
    "        with open(os.path.normpath(SAVEPATH +str(tmp_seed)+R'/plots/png/image_values/generated_images.txt'), \"a+\") as outfile:\n",
    "            # summarize discriminator performance\n",
    "            outfile.write(str(i)+\"\\n\")\n",
    "\n",
    "    # save the weights of the generator and discriminator\n",
    "    generator.save(os.path.normpath(SAVEPATH +str(tmp_seed)+'/plots/networks/generator.h5'))\n",
    "    discriminator.save(os.path.normpath(SAVEPATH +str(tmp_seed)+'/plots/networks/discriminator.h5'))\n",
    "        \n",
    "    return error_generator, error_discriminator, new_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of initializations\n",
    "number_seeds = 10\n",
    "\n",
    "all_errors_gen = []\n",
    "all_errors_dis = []\n",
    "all_colors = []\n",
    "start_alltime = time.time()\n",
    "\n",
    "# train GAN for each initialization\n",
    "for i in range(number_seeds):\n",
    "    # generate and set seed\n",
    "    seed = np.random.randint(99999999)\n",
    "    set_seed(seed)\n",
    "\n",
    "    # create path\n",
    "    path = create_folder(seed)\n",
    "\n",
    "    # setup images for this seed\n",
    "    faces = initialize_images()\n",
    "\n",
    "    # setup weights for this seed\n",
    "    dis,org_dis = setup_dis()\n",
    "    gen,org_gen = setup_gen()\n",
    "    gan = setup_gan(dis,gen)\n",
    "\n",
    "    # train the GAN for specific initialization\n",
    "    error_generator, error_discriminator, new_color = train(dis,gen,gan, org_dis, org_gen,path,seed,faces)\n",
    "    all_errors_gen.extend(error_generator)\n",
    "    all_errors_dis.extend(error_discriminator)\n",
    "    all_colors.extend(new_color)\n",
    "\n",
    "# save computation time of all seeds\n",
    "end_alltime = time.time()\n",
    "# get the execution time\n",
    "computation_time = end_alltime - start_alltime\n",
    "time_string = '>>>>>>>Complete computatiom time: %.3f seconds<<<<<<<' %np.round(computation_time,3)\n",
    "print(time_string)\n",
    "\n",
    "# save number of good and bad images\n",
    "n_green = 0\n",
    "n_yellow = 0\n",
    "n_red = 0\n",
    "for val in all_colors:\n",
    "    if(val<=1):\n",
    "        n_green +=1\n",
    "    elif(val<=2):\n",
    "        n_yellow +=1\n",
    "    else:\n",
    "        n_red+=1\n",
    "\n",
    "# save computatuion time and number images\n",
    "with open(os.path.normpath(path +str(seed) +'/../performance.txt'), \"a+\") as outfile:        \n",
    "\n",
    "    outfile.write(\"Number of great images:\"+str(n_green)+\"\\n\")\n",
    "    outfile.write(\"Number of good images:\"+str(n_yellow)+\"\\n\")\n",
    "    outfile.write(\"Number of bad images:\"+str(n_red)+\"\\n\")\n",
    "    outfile.write(time_string+\"\\n\"+\"\\n\")\n",
    "\n",
    "# plot the gan errors for all seeds    \n",
    "plt.scatter([all_errors_gen], [all_errors_dis], c= all_colors,s = 20,cmap='RdYlGn_r', vmin=0, vmax=3)\n",
    "plt.colorbar(label=\"image evaluation\")\n",
    "plt.title(\"GAN loss over seeds\")\n",
    "plt.xlabel('Generator_loss', fontsize=18)\n",
    "plt.ylabel('Discriminator_loss', fontsize=16)\n",
    "plt.savefig(os.path.normpath(path +str(seed)+'/../GAN_error_over_seeds.pdf')) \n",
    "plt.close()\n",
    "\n",
    "#\n",
    "# plot and save the losses for all seeds\n",
    "#\n",
    "\n",
    "plt.scatter([all_errors_gen], [all_errors_dis], c= all_colors,s = 20,cmap='RdYlGn_r', vmin=0, vmax=3)\n",
    "plt.colorbar(label=\"image evaluation\")\n",
    "plt.xscale('symlog')\n",
    "plt.yscale('symlog')\n",
    "plt.title(\"GAN loss over seeds in log scale\")\n",
    "plt.xlabel('Generator_loss', fontsize=18)\n",
    "plt.ylabel('Discriminator_loss', fontsize=16)\n",
    "plt.savefig(os.path.normpath(path +str(seed)+'/../GAN_error_over_seeds_logscale.pdf'))\n",
    "plt.close()\n",
    "\n",
    "#\n",
    "# save the erros into csv file to reuse them\n",
    "#\n",
    "\n",
    "with open(os.path.normpath(path +str(seed)+'/../saved_errors.csv'), 'w') as f:\n",
    "    # using csv.writer method from CSV package\n",
    "    write = csv.writer(f)\n",
    "    write.writerows([all_errors_gen])\n",
    "    write.writerows([all_errors_dis])\n",
    "    write.writerows([all_colors])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e11b55361148b6c3db81e2c737cd36da1a961dccb25eb888a6a7c970ac9dd9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
